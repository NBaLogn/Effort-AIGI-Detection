# üéØ Effort Model Fine-Tuning

This guide provides comprehensive instructions for fine-tuning the Effort model using the SVD decomposition approach.

## üöÄ Quick Start

### Commands

### finetune.sh

```bash
uv run 'DeepfakeBench/training/finetune.py' \
    --detector_config 'DeepfakeBench/training/config/detector/effort_finetune.yaml' \
    --train_dataset '[DATASET_PATH]' \
    --test_dataset '[DATASET_PATH]' \
    --pretrained_weights '[PATH_TO]/effort_clip_L14_trainOn_FaceForensic.pth'
```

### eval.sh

```bash
uv run 'DeepfakeBench/training/evaluate_finetune.py' \
    --detector_config 'DeepfakeBench/training/config/detector/effort_finetune.yaml' \
    --weights '[PATH_TO_FINETUNED_WEIGHT]'\
    --test_dataset '[DATASET_PATH]' '[DATASET_PATH]' \
    --output_dir '[PATH_TO_OUTPUT_FOLDER]'
```

### infer.sh

```bash
uv run 'DeepfakeBench/training/inference.py' \
    --detector_config \
        'DeepfakeBench/training/config/detector/effort_finetune.yaml' \
    --landmark_model \
        '[PATH_TO]/shape_predictor_81_face_landmarks.dat' \
    --weights \
        '[PATH_TO_FINETUNED_WEIGHT]' \
    --image \
        '[PATH_TO_IMAGE_FILE_OR_FOLDER]'
```

## üìã Fine-Tuning Configs

The fine-tuning configuration file (`effort_finetune.yaml`) includes optimized settings:

### Settings for batch2k

Used effort_clip_L14_trainOn_FaceForensic.pth, used 2000 extracted faces from Chameleon, Genimage, quan and quanFaceSwap datasets, finetuned for 2 epochs. 

--train_dataset and --test_dataset are the same.

### Settings for batchAll

Used effort_clip_L14_trainOn_FaceForensic.pth, used all the extracted faces from Chameleon, Genimage, quan and quanFaceSwap datasets, finetuned for 10 epochs on cosine lr_scheduler. 

--train_dataset are the train splits, --test_dataset targets the val splits.

##### Config

```yaml
# Training configuration
nEpochs: 10
lr_scheduler: cosine
lr_T_max: 10
lr_eta_min: 0.000001

# Optimizer settings
optimizer:
  type: adam
  adam:
    lr: 0.00005
    beta1: 0.9
    beta2: 0.999
    weight_decay: 0.0001

# Data augmentation settings
data_aug:
  flip_prob: 0.5
  rotate_prob: 0.4  # Slightly reduced from original 0.5
  rotate_limit: [-10, 10]  # Standard range
  blur_prob: 0.3  # Moderate blur
  blur_limit: [3, 7]  # Standard range
  brightness_prob: 0.3  # Moderate brightness variation
  brightness_limit: [-0.1, 0.1]  # Standard range
  contrast_limit: [-0.1, 0.1]  # Standard range
  quality_lower: 40  # Support lower quality samples
  quality_upper: 100  # Full quality ceiling
```

### Settings for newBatch

Added a Midjourney dataset from ivansivkovenin, implemented early stopping if AUC doesn't improve above 0.001 for 2 epochs. Reduced the learning rate.

--train_dataset are Chameleon, Genimage, ivansivkovenin's, quan and quanFS; --test_dataset targets df40. Finetuning stopped after 5 epochs.

##### Config
```yaml
# Training schedule tuned for the expanded dataset mix
nEpochs: 5
lr_scheduler: cosine
lr_T_max: 5
lr_eta_min: 0.000001

# optimizer config - Fine-tuning optimized settings
optimizer:
  adam:
    lr: 0.00001  # Lowered for stable transformer fine-tuning (1e-5)
    beta1: 0.9
    beta2: 0.999
    eps: 0.00000001
    weight_decay: 0.0001
    amsgrad: true

early_stopping:
  enabled: true
  patience: 2
  min_delta: 0.001   # stop if AUC does not improve meaningfully for two epochs
  metric: auc
```

## üîß How Fine-Tuning Works

### SVD Decomposition Approach

The Effort model uses **Orthogonal Subspace Decomposition** for efficient fine-tuning:

1. **Original Weight Matrix**: `W = U @ Œ£ @ V·µÄ`
2. **Fixed Main Components**: `W_main = U_r @ Œ£_r @ V_r·µÄ` (top r components)
3. **Trainable Residuals**: `W_residual = U_residual @ Œ£_residual @ V_residual·µÄ` (remaining components)
4. **Total Weight**: `W_total = W_main + W_residual`

### Parameter Efficiency

- **Fixed Parameters**: ~99% of original parameters (preserve pre-trained knowledge)
- **Trainable Parameters**: ~1% of original parameters (SVD residuals + classification head)
- **Total Trainable**: ~1-5% of model parameters

## üìä Evaluation Metrics

The evaluation script provides comprehensive metrics:

- **Primary Metrics**: AUC, EER, Accuracy, AP
- **Additional Metrics**: Precision, Recall, F1 Score
- **Detailed Logging**: Per-batch progress, final summary
- **Result Formats**: JSON output for easy analysis

## üîç Monitoring and Debugging

### Logging

- **Finetuning Logs**: `training/logs/finetuning.log`
- **Evaluation Logs**: `evaluation_results/evaluation.log`
- **TensorBoard**: Automatic logging of metrics

## üõ†Ô∏è Installation

### 1. Clone the Repository
```bash
git clone https://github.com/your-repo/effort-aigi-detection.git
cd effort-aigi-detection
```

### 2. Set Up Python Environment
```bash
# Install Python dependencies using uv
uv sync
```

This will install all Python dependencies listed in `pyproject.toml`, including:
- FastAPI and Uvicorn for the backend server
- PyTorch and related ML libraries
- OpenCV, dlib, and other computer vision tools
- Deepfake detection model dependencies

### 3. Set Up Frontend
```bash
cd frontend
npm install
# or
uv run npm install
```

This will install Next.js and React dependencies.

### 4. Download Required Models
The application requires specific model files:

#### Landmark Detection Model
Download the 81-landmark face shape predictor at https://github.com/codeniko/shape_predictor_81_face_landmarks

#### Deepfake Detection Weights
You need pretrained Effort model weights. Place them in the appropriate location. The file server.py looks for the weight file and landmark file, you must change the path.

## üöÄ Running the Application

### Development Mode

#### 1. Start the Backend Server
```bash
# From the project root
uv run backend/server.py
```

The backend will start on `http://0.0.0.0:8000` with:
- FastAPI REST API for deepfake detection
- CORS enabled for frontend communication
- Automatic model loading and Grad-CAM visualization
- Health check endpoint at `/health`

#### 2. Start the Frontend Development Server
```bash
cd frontend
npm run dev
# or
uv run npm run dev
```

The frontend will start on `http://localhost:3000` with:
- Hot module replacement for instant updates
- Interactive deepfake detection interface
- Image upload and analysis capabilities
- Visual Grad-CAM explanations

## üìä API Endpoints

### POST /predict
Upload an image for deepfake detection:

**Request:**
```bash
curl -X POST -F "file=@test_image.jpg" http://localhost:8000/predict
```

**Response:**
```json
{
  "label": "FAKE",
  "score": 0.95,
  "reasoning": "Suspicious textures detected around the eyes",
  "grad_cam_image": "data:image/jpeg;base64,..."
}
```

### GET /health
Check if the backend is running:
```bash
curl http://localhost:8000/health
```

## üéØ Usage

1. **Upload Images**: Drag and drop images or use the file picker
2. **View Results**: See real-time deepfake detection results
3. **Analyze Heatmaps**: Visual Grad-CAM explanations show which facial regions triggered the detection
4. **Batch Processing**: Upload multiple images for batch analysis

##  Project Structure

```
effort-aigi-detection/
‚îú‚îÄ‚îÄ backend/                  # FastAPI backend
‚îÇ   ‚îú‚îÄ‚îÄ server.py             # Main backend application
‚îÇ   ‚îî‚îÄ‚îÄ gradcam_utils.py      # Grad-CAM utilities
‚îú‚îÄ‚îÄ frontend/                 # Next.js frontend
‚îÇ   ‚îú‚îÄ‚îÄ app/                  # Application pages
‚îÇ   ‚îú‚îÄ‚îÄ components/           # React components
‚îÇ   ‚îî‚îÄ‚îÄ public/               # Static assets
‚îú‚îÄ‚îÄ DeepfakeBench/            # Core detection logic
‚îÇ   ‚îú‚îÄ‚îÄ training/             # Training scripts
‚îÇ   ‚îî‚îÄ‚îÄ preprocessing/        # Data preprocessing
‚îî‚îÄ‚îÄ README.md                 # This file
```
